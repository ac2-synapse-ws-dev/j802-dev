{
	"name": "AFRICOM_Instability_Index",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "ac2sparkpooldev",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "61bf896e-298f-4cb9-ad3f-be68d8822bc1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/fb276628-ada1-41d2-9fe6-b02c83c5d04e/resourceGroups/ac2-rg-synapse-01/providers/Microsoft.Synapse/workspaces/ac2-synapse-ws-dev/bigDataPools/ac2sparkpooldev",
				"name": "ac2sparkpooldev",
				"type": "Spark",
				"endpoint": "https://ac2-synapse-ws-dev.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/ac2sparkpooldev",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### LTC Cardy Moten III, ACJ802, cardy.moten3.mil@mail.mil \n",
					"#### 19 May 2025"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Purpose\n",
					"\n",
					"This notebook provides the methodology for extracting the requisite datasets to build and subsequently analyze the [AFRICOM Instability Risk Index (IRI) Assessment](https://dod365.sharepoint-mil.us/:w:/r/teams/AFRICOM-AFRICOMJ802Collaboration/Shared%20Documents/General/Campaign%20Assessment%20Literature/3-Analysis%20and%20Assessment/Risk%20Analysis/Africa%20Instability%20Risk%20Assessment_5MAY2025.docx?d=wc2e7e9be66ea4c1b9e77a30f729d54ce&csf=1&web=1&e=XXlM2m) [1]. The proponent for the use of the index and the methodology to create the index is the USAFRICOM J54."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Index Overview\n",
					"\n",
					"The IRI is a composite index that is built from 30 indicators across two domains (Social Cohesion and Governance Capacity) developed by the IRI assessmeent team [1]. Each indicator is converted from its raw format to a rescaled score between 0 to 2 summarized by the table below.\n",
					"\n",
					"| Score | Interpretation            |\n",
					"|-------|---------------------------|\n",
					"| 0     | low instability risk      |\n",
					"| 1     | moderate instability risk |\n",
					"| 2     | high instability risk     |\n",
					"\n",
					"Based on this method a country can have a score ranging from 0 - 60 with lower scores representing more stability. To ensure consistency with using this index with across countries when analyzing with regards to domains across time, the scores are nomalized again to a 0 - 1 ratio value. This conversion is simply done by the following formula $$normalized\\_score = \\frac{current\\_score}{max\\_score}$$\n",
					"\n",
					"## Methodology Addition\n",
					"\n",
					"In addition to the coversion of the raw data, we will also produce estimates out to the current year of analysis since a few datasets stop their collection prior to the current year of analysis. In these cases we will employ Autoregressive Integrated Moving Average (ARIMA) estimates along with building a sample distribution using Metalogs [2] [3].\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Packages"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Installs"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%pip install pandas_datareader"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Imports"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"import numpy as np\n",
					"import requests\n",
					"import pandas_datareader.wb as wb\n",
					"from notebookutils import mssparkutils\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, year"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Helper Functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def check_country_names(list_a, list_b):\n",
					"    set_a = set(list_a)\n",
					"    set_b = set(list_b)\n",
					"    print(set_a.difference(set_b))\n",
					"\n",
					"def capture_named_args(**kwargs):\n",
					"    return list(kwargs.keys())\n",
					"\n",
					"def create_domain_index(country_iso, df_1, df_2, df_3, df_4, df_5):\n",
					"    arg_names = capture_named_args(\n",
					"        country_iso=country_iso,\n",
					"        df_1=df_1,\n",
					"         df_2=df_2,\n",
					"         df_3=df_3,\n",
					"         df_4=df_4,\n",
					"         df_5=df_5\n",
					"    )\n",
					"\n",
					"    def extract_scores(df, df_name):\n",
					"        if country_iso not in df['iso3'].values:\n",
					"            raise ValueError(f\"{country_iso} not found. Check {df_name} data.\")\n",
					"        return df[df['iso3'] == country_iso][['iso3', 'name', 'year'] + [col for col in df.columns if col.endswith('_score')]]\n",
					"\n",
					"    df_1_score = extract_scores(df_1, arg_names[1])\n",
					"    df_2_score = extract_scores(df_2, arg_names[2])\n",
					"    df_3_score = extract_scores(df_3, arg_names[3])\n",
					"    df_4_score = extract_scores(df_4, arg_names[4])\n",
					"    df_5_score = extract_scores(df_5, arg_names[5])\n",
					"\n",
					"    # Merge all dataframes on 'year'\n",
					"    index_df = df_1_score.merge(df_2_score, on='year', suffixes=('', '_df2')) \\\n",
					"                         .merge(df_3_score, on='year', suffixes=('', '_df3')) \\\n",
					"                         .merge(df_4_score, on='year', suffixes=('', '_df4')) \\\n",
					"                         .merge(df_5_score, on='year', suffixes=('', '_df5'))\n",
					"\n",
					"    print(\"Current Index DF\")\n",
					"    print(index_df)\n",
					"    # Rename columns to match expected output\n",
					"    index_df.rename(columns={'iso3': 'iso3.x', 'name': 'name.x'}, inplace=True)\n",
					"\n",
					"    # Identify score columns\n",
					"    score_cols = [col for col in index_df.columns if col.endswith('_score')]\n",
					"    \n",
					"\n",
					"    # Interpolate missing values\n",
					"    for col in score_cols:\n",
					"        if index_df[col].isnull().any():\n",
					"            index_df[col] = index_df[col].interpolate(method='linear', limit_direction='both')\n",
					"\n",
					"    # Calculate domain score\n",
					"    index_df['domain_score'] = index_df[score_cols].sum(axis=1, skipna=True) / 10\n",
					"    index_df = index_df[['iso3.x', 'name.x', 'year', 'domain_score']]\n",
					"\n",
					"    return index_df"
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Gather Data\n",
					"\n",
					"USAFRICOM should collect new data on an annual basis since most of these data sources are updated on that cycle.\n",
					"\n",
					"## Social Cohesion Data\n",
					"\n",
					"| Data Source | URL | Indicator Domain | Notes |\n",
					"|-------------|-----|------------------|-------|\n",
					"| Notre Dame Global Adaptation Initiative (ND-GAIN) [4]    | <https://gain.nd.edu/our-work/country-index/download-data/> | Socioenvironmental Pressures | Annual Update |\n",
					"| The Commonwealth - Global Youth Index [5]    | <https://cwapiservices.thecommonwealth.org/index.html> | Socioenvironmental Pressures | Annual Update |\n",
					"| World Bank Literacy Rates [6]    | <https://data.worldbank.org/indicator/SE.ADT.LITR.ZS?most_recent_value_desc=true> | Socioenvironmental Pressures | Replacement for the United Nations Human Development Report -Education Index Score |\n",
					"| Fragile State Index—Human Flight and Brain Drain score [7]    | <https://fragilestatesindex.org/indicators/e3/> | Socioenvironmental Pressures | Annual Update |\n",
					"| Fragile State Index—Demographic Pressures score [7]   | <https://fragilestatesindex.org/indicators/s1/> | Socioenvironmental Pressures | Annual Update |\n",
					"\n",
					"Table of IRI Social Cohesion Datasources\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Socioenvironmental Pressures Domain"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### ND-Gain\n",
					"\n",
					"This dataset comes in a zip file containing multiple file. The required file to build the IRI is called `resources/gain/gain.csv`. After downloading the file, I renamed it to `nd_gain_country_index_2024.csv`."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Define the column names\n",
					"iri_col_names = [\"iso3\", \"name\", \"year\", \"value\"]\n",
					"\n",
					"# Read the CSV files\n",
					"environmental_adaptation_path = 'abfss://acj802@ac2synapsedatalabstorage.dfs.core.windows.net/AFRICOM_Instability_Index/nd_gain_country_index_2024.csv'\n",
					"african_countries_path = 'abfss://acj802@ac2synapsedatalabstorage.dfs.core.windows.net/AFRICOM_Instability_Index/African_Countries_ISO3.csv'\n",
					"environmental_adaptation = pd.read_csv(environmental_adaptation_path)\n",
					"\n",
					"african_countries = pd.read_csv(african_countries_path)\n",
					"african_countries.loc[african_countries['country'] == \"Côte d'Ivoire\", 'country'] = \"Cote d'Ivoire\"\n",
					"\n",
					"# Filter for African countries\n",
					"environmental_adaptation = environmental_adaptation[environmental_adaptation['ISO3'].isin(african_countries['iso3'])]\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Congo\", 'Name'] = \"Republic of the Congo\"\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Congo, the Democratic Republic o\", 'Name'] = \"Democratic Republic of the Congo\"\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Cape Verde\", 'Name'] = \"Cabo Verde\"\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Libyan Arab Jamahiriya\", 'Name'] = \"Libya\"\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Swaziland\", 'Name'] = \"Eswatini\"\n",
					"environmental_adaptation.loc[environmental_adaptation['Name'] == \"Tanzania, United Republic of\", 'Name'] = \"Tanzania\"\n",
					"\n",
					"\n",
					"# Reshape the data from wide to long format\n",
					"environmental_adaptation = environmental_adaptation.melt(\n",
					"    id_vars=[\"ISO3\", \"Name\"],\n",
					"    value_vars=[str(year) for year in range(1995, 2022 + 1)],\n",
					"    var_name=\"year\",\n",
					"    value_name=\"value\"\n",
					")\n",
					"\n",
					"# Rename columns\n",
					"environmental_adaptation.columns = iri_col_names\n",
					"\n",
					"# Compute 'adaptation_score'\n",
					"environmental_adaptation['adaptation_score'] = 2 - (2 * environmental_adaptation['value'] / 100)\n",
					"\n",
					"# Convert 'year' to numeric\n",
					"environmental_adaptation['year'] = pd.to_numeric(environmental_adaptation['year'])\n",
					"\n",
					"# Filter for years >= 2000\n",
					"environmental_adaptation = environmental_adaptation[environmental_adaptation['year'] >= 2000]\n",
					"\n",
					"print(environmental_adaptation.head())"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"check_country_names(environmental_adaptation['name'], african_countries['country']) #used to check for spelling differences in country name sets"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Commonwealth Youth Index"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Read in json file\n",
					"\n",
					"url = \"https://cwapiservices.thecommonwealth.org/CountryData/GetAllYDI_index_scores\"\n",
					"headers = {\"Accept\": \"application/json\"}\n",
					"\n",
					"response = requests.get(url, headers=headers)\n",
					"data = response.json()\n",
					"\n",
					"\n",
					"# Convert to DataFrame\n",
					"youth_underdevelopment = pd.json_normalize(data)\n",
					"youth_underdevelopment.loc[youth_underdevelopment['country'] == 'C�te d�Ivoire', 'country'] = \"Cote d'Ivoire\"\n",
					"youth_underdevelopment.loc[youth_underdevelopment['country'] == 'S�o Tom� & Pr�ncipe', 'country'] = \"Sao Tome and Principe\"\n",
					"youth_underdevelopment.loc[youth_underdevelopment['country'] =='Congo - Kinshasa', 'country'] = \"Democratic Republic of the Congo\"\n",
					"youth_underdevelopment.loc[youth_underdevelopment['country'] == 'Congo - Brazzaville', 'country'] = \"Republic of the Congo\"\n",
					"youth_underdevelopment.loc[youth_underdevelopment['country'] == 'Cape Verde', 'country'] = \"Cabo Verde\"\n",
					"\n",
					"# Filter for African countries\n",
					"youth_underdevelopment = youth_underdevelopment[youth_underdevelopment['iD_0'].isin(african_countries['iso3'])]\n",
					"\n",
					"#Select iD_0, country, year, overall_score\n",
					"youth_underdevelopment = youth_underdevelopment[['iD_0', 'country', 'year', 'overall_score']]\n",
					"\n",
					"# Rename columns\n",
					"youth_underdevelopment.columns = iri_col_names\n",
					"\n",
					"# Compute 'underdevelopment_score'\n",
					"youth_underdevelopment['underdevelopment_score'] = 2 - (2 * youth_underdevelopment['value'])\n",
					"\n",
					"# Convert 'year' to numeric\n",
					"youth_underdevelopment['year'] = pd.to_numeric(youth_underdevelopment['year'])\n",
					"\n",
					"# Filter for years >= 2000\n",
					"youth_underdevelopment = youth_underdevelopment[youth_underdevelopment['year'] >= 2000]\n",
					"\n",
					"print(youth_underdevelopment.head())\n",
					"\n",
					""
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"check_country_names(youth_underdevelopment['name'], african_countries['country'])"
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Illiteracy"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Read in Data\n",
					"\n",
					"# Define the indicator and countries (or use 'all')\n",
					"indicator = 'SE.ADT.LITR.ZS'\n",
					"\n",
					"# Fetch data for all countries\n",
					"illiteracy = wb.download(indicator=indicator, country='all', start=2000, end=2023)\n",
					"illiteracy = illiteracy.reset_index()\n",
					"illiteracy.loc[illiteracy['country'] == 'Congo, Dem. Rep.', 'country'] = 'Democratic Republic of the Congo'\n",
					"illiteracy.loc[illiteracy['country'] == 'Congo, Rep.', 'country'] = 'Republic of the Congo'\n",
					"\n",
					"\n",
					"\n",
					"#Get ISO3 values and filter for specific African countries\n",
					"illiteracy = pd.merge(illiteracy, african_countries, on='country', how='left')\n",
					"illiteracy = illiteracy[illiteracy['iso3'].isin(african_countries['iso3'])]\n",
					"\n",
					"#Rename columns\n",
					"illiteracy = illiteracy[['iso3', 'country', 'year', indicator]]\n",
					"illiteracy.columns = iri_col_names\n",
					"illiteracy = illiteracy.dropna(subset=['value'])\n",
					"illiteracy['value'] = 100 - illiteracy['value'] #want illiteracy percentage\n",
					"\n",
					"# Compute 'illiteracy_score'\n",
					"illiteracy['illiteracy_score'] = 2 * illiteracy['value'] / 100\n",
					"\n",
					"# Convert 'year' to numeric\n",
					"illiteracy['year'] = pd.to_numeric(illiteracy['year'])\n",
					"\n",
					"# Filter for years >= 2000\n",
					"illiteracy = illiteracy[illiteracy['year'] >= 2000]\n",
					"\n",
					"\n",
					"print(illiteracy.head())\n",
					""
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"check_country_names(illiteracy['name'], african_countries['country'])"
				],
				"execution_count": 26
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Fragile State Index Data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"file_list = mssparkutils.fs.ls(\"abfss://acj802@ac2synapsedatalabstorage.dfs.core.windows.net/AFRICOM_Instability_Index/fsi_data\")\n",
					"file_paths = []\n",
					"\n",
					"for file_path in file_list:\n",
					"    if '.xlsx' in file_path.path:\n",
					"        file_paths.append(file_path.path)\n",
					"\n",
					"dfs = []\n",
					"\n",
					"\n",
					"for path in file_paths:\n",
					"    df = spark.read.format(\"com.crealytics.spark.excel\") \\\n",
					"        .option(\"header\", \"true\") \\\n",
					"        .option(\"inferSchema\", \"true\") \\\n",
					"        .load(path)\n",
					"    df = df.select(df.columns[:16]) #Data from 2009 onwards has an extra column\n",
					"    \n",
					"# Cast all columns to string to ensure compatibility\n",
					"    for col_name in df.columns:\n",
					"        df = df.withColumn(col_name, df[col_name].cast(\"string\"))\n",
					"\n",
					"    dfs.append(df)\n",
					"\n",
					"# Union all DataFrames\n",
					"combined_df = dfs[0]\n",
					"for df in dfs[1:]:\n",
					"    combined_df = combined_df.unionByName(df)\n",
					"\n",
					"\n",
					"# Cast the first three columns\n",
					"fsi_data = combined_df \\\n",
					"    .withColumn(\"Country\", col(\"Country\").cast(\"string\")) \\\n",
					"    .withColumn(\"Year\", year(col(\"Year\")).cast(\"int\")) \\\n",
					"    .withColumn(\"Rank\", col(\"Rank\").cast(\"string\"))\n",
					"\n",
					"\n",
					"# Cast the remaining columns to double\n",
					"for col_name in fsi_data.columns[3:]:\n",
					"    fsi_data = fsi_data.withColumn(col_name, col(col_name).cast(\"double\"))\n",
					"\n",
					"fsi_data = fsi_data.toPandas()\n",
					"fsi_data[\"Year\"] = fsi_data[\"Year\"].fillna(0).astype(int)\n",
					"\n",
					"\n",
					"print(fsi_data.head())\n",
					""
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Human Flight (FSI E3)\n",
					"\n",
					"This is a subsection of the `fsi` dataset, thus a similar methodology will be used for other `fsi` subsets."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Extract appropriate column and format the rest of the data\n",
					"selected_col = 'E3: Human Flight and Brain Drain'\n",
					"human_flight = fsi_data[['Country', 'Year', selected_col]]\n",
					"human_flight.loc[human_flight['Country'] == 'Congo Democratic Republic', 'Country'] = 'Democratic Republic of the Congo'\n",
					"human_flight = pd.merge(human_flight, african_countries, left_on='Country',  right_on='country', how='left')\n",
					"human_flight = human_flight[human_flight['iso3'].isin(african_countries['iso3'])]\n",
					"human_flight = human_flight[['iso3', 'Country', 'Year', selected_col]]\n",
					"human_flight.columns = iri_col_names\n",
					"human_flight = human_flight.drop_duplicates(subset=['iso3', 'year'], keep=\"first\")\n",
					"\n",
					"# Compute 'human_flight_score'\n",
					"human_flight['human_flight_score'] = 2 * human_flight['value'] / 10\n",
					"\n",
					"# Convert 'year' to numeric\n",
					"human_flight['year'] = pd.to_numeric(human_flight['year'])\n",
					"\n",
					"# Filter for years >= 2000\n",
					"human_flight = human_flight[human_flight['year'] >= 2000]\n",
					"\n",
					"print(human_flight.head())"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"check_country_names(human_flight['name'], african_countries['country'])"
				],
				"execution_count": 43
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Demographic Pressures (S1)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"#Extract appropriate column and format the rest of the data\n",
					"selected_col = 'S1: Demographic Pressures'\n",
					"demographic_pressure = fsi_data[['Country', 'Year', selected_col]]\n",
					"demographic_pressure.loc[demographic_pressure['Country'] == 'Congo Democratic Republic', 'Country'] = 'Democratic Republic of the Congo'\n",
					"demographic_pressure = pd.merge(demographic_pressure, african_countries, left_on='Country',  right_on='country', how='left')\n",
					"demographic_pressure = demographic_pressure[demographic_pressure['iso3'].isin(african_countries['iso3'])]\n",
					"demographic_pressure = demographic_pressure[['iso3', 'Country', 'Year', selected_col]]\n",
					"demographic_pressure.columns = iri_col_names\n",
					"demographic_pressure = demographic_pressure.drop_duplicates(subset=['iso3', 'year'], keep=\"first\")\n",
					"\n",
					"# Compute 'demographic_pressure_score'\n",
					"demographic_pressure['demographic_pressure_score'] = 2 * demographic_pressure['value'] / 10\n",
					"\n",
					"# Convert 'year' to numeric\n",
					"demographic_pressure['year'] = pd.to_numeric(demographic_pressure['year'])\n",
					"\n",
					"# Filter for years >= 2000\n",
					"demographic_pressure = demographic_pressure[demographic_pressure['year'] >= 2000]\n",
					"\n",
					"print(demographic_pressure.head())"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"check_country_names(demographic_pressure['name'], african_countries['country'])"
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Socioenvironmental Pressures Index"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"def capture_original_variable_names(var_dict):\n",
					"    df_values = [value for value in var_dict.values() if isinstance(value, pd.DataFrame)]\n",
					"    matched_names = []\n",
					"\n",
					"    for name, obj in globals().items():\n",
					"        if isinstance(obj, pd.DataFrame) and any(obj is val for val in df_values):\n",
					"            matched_names.append(name)\n",
					"\n",
					"    return matched_names\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"# Dynamically passing variables\n",
					"variables = {\n",
					"    \"df1\": environmental_adaptation,\n",
					"    \"df2\": youth_underdevelopment,\n",
					"    \"df3\": illiteracy,\n",
					"    \"df4\": human_flight,\n",
					"    \"df5\": demographic_pressure\n",
					"}\n",
					"\n",
					"df_values = [value for value in variables.values() if isinstance(value, pd.DataFrame)]\n",
					"for name, obj in globals().items():\n",
					"    if isinstance(obj, pd.DataFrame) and any(obj is val for val in df_values):\n",
					"        print(name)\n",
					""
				],
				"execution_count": 73
			},
			{
				"cell_type": "code",
				"source": [
					"df_names = [name for name, obj in globals().items() if isinstance(obj, pd.DataFrame)]\n",
					"print(df_names)"
				],
				"execution_count": 63
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\n",
					"### References\n",
					"[1] USAFRICOM J54. (2025). *Africa Instability Risk Assessment*. USAFRICOM.\n",
					"\n",
					"[2] Wikipedia. (2025). *Autoregressive integrated moving average*. http://en.wikipedia.org/w/index.php?title=Autoregressive\\%20integrated\\%20moving\\%20average&oldid=1286449544, accessed 07 May 2025.\n",
					"\n",
					"[3] Wikipedia. (2025). *Metalog distribution*. http://en.wikipedia.org/w/index.php?title=Metalog\\%20distribution&oldid=1278020484, accessed 07 May 2025.\n",
					"\n",
					"[4] University of Notre Dame. (2025). *Notre Dame Global Adaptation Initiative Country Index (ND-GAIN)*. https://gain.nd.edu/our-work/country-index/download-data/\n",
					"\n",
					"[5] The Commonwealth. (2025). *Youth Development Index*. https://cwapiservices.thecommonwealth.org/index.html\n",
					"\n",
					"[6] World Bank. (2025). *Literacy rate, adult total (% of people ages 15 and above)*. https://data.worldbank.org/indicator/SE.ADT.LITR.ZS?most_recent_value_desc=true\n",
					"\n",
					"[7] The Fund for Peace. (2022). *Fragile State Index*. https://fragilestatesindex.org/excel/\n",
					"\n",
					"[8] Next Reference\n",
					"\n",
					""
				]
			}
		]
	}
}